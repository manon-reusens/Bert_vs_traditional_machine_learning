{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import copy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_annotations=  pd.read_csv('full_cleaned_dataset.csv')\n",
    "df_annotations.head() # to display the first 5 lines of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(data, max_feat, n_gram):\n",
    "    vect=TfidfVectorizer(max_features=max_feat, stop_words= stopwords.words('dutch'), token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b', ngram_range=n_gram).fit(data)\n",
    "    X= vect.fit_transform(data)\n",
    "    X=pd.DataFrame(X.toarray(), columns= vect.get_feature_names())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW(data, ngram, max_features=100):\n",
    "    vect=CountVectorizer(max_features, ngram_range=ngram)\n",
    "    vect.fit(data)\n",
    "    X_review=vect.transform(data)\n",
    "\n",
    "    X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_set(dataset_column):\n",
    "    y= dataset_column\n",
    "    y = y.replace('NEUTRAL','1')\n",
    "    y = y.replace('POSITIVE','2')\n",
    "    y = y.replace('NEGATIVE','0')\n",
    "    for i in y:\n",
    "        i=int(i)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_mat(y_test, y_pred):\n",
    "    mat=confusion_matrix(y_test, y_pred)/len(y_test)\n",
    "    return sns.heatmap(mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X, y_inputkolom, reg_term=1):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Build a logistic regression\n",
    "    log_reg = LogisticRegression(max_iter=500, C= reg_term).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = log_reg.predict(X_test)\n",
    "    new=[]\n",
    "    for i in y_test:\n",
    "        new.append(int(i))\n",
    "        \n",
    "    y_pred=[]\n",
    "    for i in y_predict:\n",
    "        y_pred.append(int(i))\n",
    "    print(' The Macro F1 measure is', metrics.f1_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro precision is',metrics.precision_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro recall is',metrics.recall_score(new, y_pred, average='macro'))\n",
    "        \n",
    "    print('The Accuracy of the model is', log_reg.score(X_test, y_test))\n",
    "    print('The confusion matrix looks as follows', cf_mat(y_test, y_predict))\n",
    "    \n",
    "    \n",
    "    #return y_predict, y_test, log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_val(X,y_inputkolom):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "\n",
    "    # Build a logistic regression\n",
    "    log_reg = LogisticRegression(max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = log_reg.predict(X_val)\n",
    "    acc_score= log_reg.score(X_val, y_val)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_val_reg(X,y_inputkolom, reg_term, max_it=500):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "\n",
    "    # Build a logistic regression\n",
    "    log_reg = LogisticRegression(max_iter=max_it, C=reg_term).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = log_reg.predict(X_val)\n",
    "    acc_score= log_reg.score(X_val, y_val)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    #print('The confusion matrix looks as follows', cf_mat(y_val, y_predict))\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.text.values.astype('U'),i,(1,1))\n",
    "    acc_score=logreg_val(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2)]:\n",
    "    X=TFIDF(df_annotations.text.values.astype('U'),6000,i)\n",
    "    logreg_val(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing the regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "X= TFIDF(df_annotations.text.values.astype('U'),6000,(1,1))\n",
    "for i in max_feat_list:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF and then logreg with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X=TFIDF(df_annotations.text.values.astype('U'),6000,(1,1))\n",
    "logreg(X,df_annotations['label'], 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),i,(1,1))\n",
    "    acc_score=logreg_val(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2)]:\n",
    "    X=TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),6000,i)\n",
    "    logreg_val(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing the regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "X= TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),6000,(1,1))\n",
    "for i in max_feat_list:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF with optimal parameters with extra preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X= TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),6000, (1,1))\n",
    "logreg(X, df_annotations['label'], 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.processed_annotations_lemmatized_joined.values.astype('U'),i,(1,1))\n",
    "    acc_score=logreg_val(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2)]:\n",
    "    X=TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),10000,i)\n",
    "    logreg_val(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "for i in max_feat_list:\n",
    "    print('the regularisation term is', i)\n",
    "    X= TFIDF(df_annotations.processed_annotations_lemmatized_joined.astype('U'),10000,(1,1))\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= TFIDF(df_annotations.processed_annotations_lemmatized_joined.values.astype('U'),10000, (1,1))\n",
    "logreg(X, df_annotations['label'],0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec selftrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent(data_column):\n",
    "    sentences=[]\n",
    "    for i in range(len(df_annotations['text'])):\n",
    "        words_in_sentence=df_annotations['text'][i].split()\n",
    "        sentences.append(words_in_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V(sent, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025):\n",
    "    model= Word2Vec(sentences=sent, vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(df_annotations['text'])):\n",
    "        l=[]\n",
    "        for word in df_annotations['text'][i].split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                l.append(list(model.wv.get_vector(word)))\n",
    "            else:\n",
    "                l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        #a = np.array(l)\n",
    "        #print(a)\n",
    "        #res = np.average(a, axis=0)\n",
    "        #lijst=list(res)\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000]\n",
    "for i in max_vect:\n",
    "    X=W2V(sent(df_annotations['text']), i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we choose vector_size= 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20]\n",
    "for i in wind:\n",
    "    X=W2V(sent(df_annotations['text']), 200, i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we kiezen window 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20,30,50]\n",
    "for i in min_c:\n",
    "    X=W2V(sent(df_annotations['text']), 200, 15,i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dus we kiezen min_count 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    X=W2V(sent(df_annotations['text']), 200, 15,2,16,i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dus sgd=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1]\n",
    "for i in alphas:\n",
    "    X=W2V(sent(df_annotations['text']), 200, 15,2,16,1,i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus we kiezen 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "reg_term=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "for i in reg_term:\n",
    "    print('the regularisation term is', i)\n",
    "    X=W2V(sent(df_annotations['text']), 200, 15,2,16,1,0.15)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(reg_term, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V(sent(df_annotations['text']), 200, 15,2,16,1,0.15)\n",
    "acc_score=logreg(X, df_annotations['label'], 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_lem(sent, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025):\n",
    "    model= Word2Vec(sentences=sent, vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(df_annotations['processed_annotations_lemmatized_joined'])):\n",
    "        l=[]\n",
    "        if type(df_annotations['processed_annotations_lemmatized_joined'][i])== float:\n",
    "            l.append([0]*vector_s)\n",
    "        else:\n",
    "            for word in df_annotations['processed_annotations_lemmatized_joined'][i].split():\n",
    "                if word in model.wv.key_to_index:\n",
    "                    l.append(list(model.wv.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000]\n",
    "for i in max_vect:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20]\n",
    "for i in wind:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 300, i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20,30,50]\n",
    "for i in min_c:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 300, 12,i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 300, 12,5,16,i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1, 1.2]\n",
    "for i in alphas:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 300, 12,5,16,1,i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], 1, 1000)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "reg_term=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 300, 12,5,16,1,0.1)\n",
    "for i in reg_term:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(reg_term, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 300, 12,5,16,1,0.1)\n",
    "acc_score=logreg(X, df_annotations['label'], 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_pretrained_emb(data):\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(data)):\n",
    "        l=[]\n",
    "        \n",
    "        if type(data[i])== float:\n",
    "            l.append([0]*300)\n",
    "        else:\n",
    "            for word in data[i].split():\n",
    "                if word in model.key_to_index:\n",
    "                    l.append(list(model.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*300)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "reg_term=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "for i in reg_term:\n",
    "    print('the regularisation term is', i)\n",
    "    X=W2V_pretrained_emb(df_annotations['text'])\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(reg_term, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['text'])\n",
    "acc_score=logreg(X, df_annotations['label'], 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "reg_term=[0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "for i in reg_term:\n",
    "    print('the regularisation term is', i)\n",
    "    X=W2V_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(reg_term, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "acc_score=logreg(X, df_annotations['label'], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext using pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://fasttext.cc/docs/en/support.html#building-fasttext-python-module\n",
    "#https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext.util.download_model('nl', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('cc.nl.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_pretrained_emb(data):\n",
    "    fasttext_text_av=[]\n",
    "    for i in range(len(data)):\n",
    "        l=[]\n",
    "        \n",
    "        if type(data[i])== float:\n",
    "            l.append([0]*300)\n",
    "        else:\n",
    "            for word in data[i].split():\n",
    "                l.append(list(model.get_word_vector(word)))\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        fasttext_text_av.append(avg)\n",
    "    X=pd.DataFrame(fasttext_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "acc_score=logreg_val_reg(X, df_annotations['label'], 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "reg_term=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "for i in reg_term:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "reg_term=[1,1.2,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3]\n",
    "X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "for i in reg_term:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(reg_term, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "acc_score=logreg(X, df_annotations['label'], 2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "reg_term=[0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1, 1.5, 2, 2.5, 3]\n",
    "X=fasttext_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "for i in reg_term:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=logreg_val_reg(X, df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(reg_term, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "acc_score=logreg(X, df_annotations['label'], 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
